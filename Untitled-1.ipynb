{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(2,)\n",
      "Iteration 1, loss = 0.75921947\n",
      "Iteration 2, loss = 0.75848089\n",
      "Iteration 3, loss = 0.75742916\n",
      "Iteration 4, loss = 0.75609736\n",
      "Iteration 5, loss = 0.75451559\n",
      "Iteration 6, loss = 0.75271118\n",
      "Iteration 7, loss = 0.75070895\n",
      "Iteration 8, loss = 0.74853136\n",
      "Iteration 9, loss = 0.74619876\n",
      "Iteration 10, loss = 0.74372952\n",
      "Iteration 11, loss = 0.74114024\n",
      "Iteration 12, loss = 0.73844588\n",
      "Iteration 13, loss = 0.73565993\n",
      "Iteration 14, loss = 0.73279453\n",
      "Iteration 15, loss = 0.72986057\n",
      "Iteration 16, loss = 0.72686787\n",
      "Iteration 17, loss = 0.72382520\n",
      "Iteration 18, loss = 0.72074046\n",
      "Iteration 19, loss = 0.71762069\n",
      "Iteration 20, loss = 0.71447221\n",
      "Iteration 21, loss = 0.71130066\n",
      "Iteration 22, loss = 0.70811107\n",
      "Iteration 23, loss = 0.70490792\n",
      "Iteration 24, loss = 0.70169521\n",
      "Iteration 25, loss = 0.69847649\n",
      "Iteration 26, loss = 0.69525490\n",
      "Iteration 27, loss = 0.69203324\n",
      "Iteration 28, loss = 0.68881399\n",
      "Iteration 29, loss = 0.68559931\n",
      "Iteration 30, loss = 0.68239114\n",
      "Iteration 31, loss = 0.67919117\n",
      "Iteration 32, loss = 0.67600086\n",
      "Iteration 33, loss = 0.67282153\n",
      "Iteration 34, loss = 0.66965429\n",
      "Iteration 35, loss = 0.66650014\n",
      "Iteration 36, loss = 0.66335991\n",
      "Iteration 37, loss = 0.66023433\n",
      "Iteration 38, loss = 0.65712403\n",
      "Iteration 39, loss = 0.65402953\n",
      "Iteration 40, loss = 0.65095129\n",
      "Iteration 41, loss = 0.64788967\n",
      "Iteration 42, loss = 0.64484498\n",
      "Iteration 43, loss = 0.64181746\n",
      "Iteration 44, loss = 0.63880732\n",
      "Iteration 45, loss = 0.63581469\n",
      "Iteration 46, loss = 0.63283970\n",
      "Iteration 47, loss = 0.62988242\n",
      "Iteration 48, loss = 0.62694290\n",
      "Iteration 49, loss = 0.62402114\n",
      "Iteration 50, loss = 0.62111715\n",
      "Iteration 51, loss = 0.61823090\n",
      "Iteration 52, loss = 0.61536235\n",
      "Iteration 53, loss = 0.61251143\n",
      "Iteration 54, loss = 0.60967808\n",
      "Iteration 55, loss = 0.60686222\n",
      "Iteration 56, loss = 0.60406376\n",
      "Iteration 57, loss = 0.60128258\n",
      "Iteration 58, loss = 0.59851860\n",
      "Iteration 59, loss = 0.59577170\n",
      "Iteration 60, loss = 0.59304176\n",
      "Iteration 61, loss = 0.59032867\n",
      "Iteration 62, loss = 0.58763231\n",
      "Iteration 63, loss = 0.58495256\n",
      "Iteration 64, loss = 0.58228929\n",
      "Iteration 65, loss = 0.57964239\n",
      "Iteration 66, loss = 0.57701172\n",
      "Iteration 67, loss = 0.57439718\n",
      "Iteration 68, loss = 0.57179863\n",
      "Iteration 69, loss = 0.56921596\n",
      "Iteration 70, loss = 0.56664904\n",
      "Iteration 71, loss = 0.56409777\n",
      "Iteration 72, loss = 0.56156203\n",
      "Iteration 73, loss = 0.55904170\n",
      "Iteration 74, loss = 0.55653668\n",
      "Iteration 75, loss = 0.55404685\n",
      "Iteration 76, loss = 0.55157211\n",
      "Iteration 77, loss = 0.54911235\n",
      "Iteration 78, loss = 0.54666748\n",
      "Iteration 79, loss = 0.54428155\n",
      "Iteration 80, loss = 0.54200135\n",
      "Iteration 81, loss = 0.53971989\n",
      "Iteration 82, loss = 0.53743834\n",
      "Iteration 83, loss = 0.53515775\n",
      "Iteration 84, loss = 0.53287908\n",
      "Iteration 85, loss = 0.53060320\n",
      "Iteration 86, loss = 0.52833090\n",
      "Iteration 87, loss = 0.52606289\n",
      "Iteration 88, loss = 0.52379981\n",
      "Iteration 89, loss = 0.52154223\n",
      "Iteration 90, loss = 0.52032708\n",
      "Iteration 91, loss = 0.51902690\n",
      "Iteration 92, loss = 0.51759811\n",
      "Iteration 93, loss = 0.51605022\n",
      "Iteration 94, loss = 0.51439260\n",
      "Iteration 95, loss = 0.51263434\n",
      "Iteration 96, loss = 0.51078417\n",
      "Iteration 97, loss = 0.50885044\n",
      "Iteration 98, loss = 0.50684102\n",
      "Iteration 99, loss = 0.50510863\n",
      "Iteration 100, loss = 0.50360915\n",
      "Iteration 101, loss = 0.50205027\n",
      "Iteration 102, loss = 0.50043911\n",
      "Iteration 103, loss = 0.49878209\n",
      "Iteration 104, loss = 0.49708503\n",
      "Iteration 105, loss = 0.49535317\n",
      "Iteration 106, loss = 0.49402794\n",
      "Iteration 107, loss = 0.49239834\n",
      "Iteration 108, loss = 0.49065656\n",
      "Iteration 109, loss = 0.48919722\n",
      "Iteration 110, loss = 0.48763055\n",
      "Iteration 111, loss = 0.48602111\n",
      "Iteration 112, loss = 0.48440074\n",
      "Iteration 113, loss = 0.48293896\n",
      "Iteration 114, loss = 0.48133475\n",
      "Iteration 115, loss = 0.47990765\n",
      "Iteration 116, loss = 0.47826978\n",
      "Iteration 117, loss = 0.47682772\n",
      "Iteration 118, loss = 0.47529442\n",
      "Iteration 119, loss = 0.47374236\n",
      "Iteration 120, loss = 0.47232219\n",
      "Iteration 121, loss = 0.47077883\n",
      "Iteration 122, loss = 0.46923593\n",
      "Iteration 123, loss = 0.46792363\n",
      "Iteration 124, loss = 0.46629591\n",
      "Iteration 125, loss = 0.46489083\n",
      "Iteration 126, loss = 0.46345113\n",
      "Iteration 127, loss = 0.46198913\n",
      "Iteration 128, loss = 0.46050137\n",
      "Iteration 129, loss = 0.45914557\n",
      "Iteration 130, loss = 0.45764911\n",
      "Iteration 131, loss = 0.45618397\n",
      "Iteration 132, loss = 0.45489767\n",
      "Iteration 133, loss = 0.45345939\n",
      "Iteration 134, loss = 0.45199940\n",
      "Iteration 135, loss = 0.45060589\n",
      "Iteration 136, loss = 0.44923423\n",
      "Iteration 137, loss = 0.44779491\n",
      "Iteration 138, loss = 0.44650288\n",
      "Iteration 139, loss = 0.44506799\n",
      "Iteration 140, loss = 0.44364958\n",
      "Iteration 141, loss = 0.44246217\n",
      "Iteration 142, loss = 0.44099272\n",
      "Iteration 143, loss = 0.43960791\n",
      "Iteration 144, loss = 0.43832400\n",
      "Iteration 145, loss = 0.43698783\n",
      "Iteration 146, loss = 0.43562251\n",
      "Iteration 147, loss = 0.43426392\n",
      "Iteration 148, loss = 0.43306917\n",
      "Iteration 149, loss = 0.43173592\n",
      "Iteration 150, loss = 0.43037140\n",
      "Iteration 151, loss = 0.42907428\n",
      "Iteration 152, loss = 0.42780215\n",
      "Iteration 153, loss = 0.42646746\n",
      "Iteration 154, loss = 0.42521984\n",
      "Iteration 155, loss = 0.42395576\n",
      "Iteration 156, loss = 0.42265626\n",
      "Iteration 157, loss = 0.42134101\n",
      "Iteration 158, loss = 0.42026353\n",
      "Iteration 159, loss = 0.41886012\n",
      "Iteration 160, loss = 0.41758368\n",
      "Iteration 161, loss = 0.41649197\n",
      "Iteration 162, loss = 0.41525371\n",
      "Iteration 163, loss = 0.41399272\n",
      "Iteration 164, loss = 0.41271696\n",
      "Iteration 165, loss = 0.41150453\n",
      "Iteration 166, loss = 0.41031903\n",
      "Iteration 167, loss = 0.40907453\n",
      "Iteration 168, loss = 0.40787666\n",
      "Iteration 169, loss = 0.40677195\n",
      "Iteration 170, loss = 0.40557139\n",
      "Iteration 171, loss = 0.40434366\n",
      "Iteration 172, loss = 0.40312632\n",
      "Iteration 173, loss = 0.40205265\n",
      "Iteration 174, loss = 0.40086187\n",
      "Iteration 175, loss = 0.39964624\n",
      "Iteration 176, loss = 0.39858503\n",
      "Iteration 177, loss = 0.39738428\n",
      "Iteration 178, loss = 0.39621065\n",
      "Iteration 179, loss = 0.39510607\n",
      "Iteration 180, loss = 0.39405027\n",
      "Iteration 181, loss = 0.39292302\n",
      "Iteration 182, loss = 0.39176985\n",
      "Iteration 183, loss = 0.39059433\n",
      "Iteration 184, loss = 0.38960746\n",
      "Iteration 185, loss = 0.38844048\n",
      "Iteration 186, loss = 0.38732190\n",
      "Iteration 187, loss = 0.38618051\n",
      "Iteration 188, loss = 0.38530264\n",
      "Iteration 189, loss = 0.38406821\n",
      "Iteration 190, loss = 0.38297181\n",
      "Iteration 191, loss = 0.38192831\n",
      "Iteration 192, loss = 0.38094105\n",
      "Iteration 193, loss = 0.37988238\n",
      "Iteration 194, loss = 0.37879989\n",
      "Iteration 195, loss = 0.37772996\n",
      "Iteration 196, loss = 0.37667519\n",
      "Iteration 197, loss = 0.37572046\n",
      "Iteration 198, loss = 0.37468508\n",
      "Iteration 199, loss = 0.37362715\n",
      "Iteration 200, loss = 0.37254985\n",
      "Iteration 201, loss = 0.37172200\n",
      "Iteration 202, loss = 0.37056975\n",
      "Iteration 203, loss = 0.36954350\n",
      "Iteration 204, loss = 0.36855967\n",
      "Iteration 205, loss = 0.36765471\n",
      "Iteration 206, loss = 0.36667010\n",
      "Iteration 207, loss = 0.36566336\n",
      "Iteration 208, loss = 0.36463758\n",
      "Iteration 209, loss = 0.36359553\n",
      "Iteration 210, loss = 0.36292330\n",
      "Iteration 211, loss = 0.36169049\n",
      "Iteration 212, loss = 0.36073012\n",
      "Iteration 213, loss = 0.35987239\n",
      "Iteration 214, loss = 0.35893264\n",
      "Iteration 215, loss = 0.35800139\n",
      "Iteration 216, loss = 0.35704863\n",
      "Iteration 217, loss = 0.35607733\n",
      "Iteration 218, loss = 0.35511112\n",
      "Iteration 219, loss = 0.35433034\n",
      "Iteration 220, loss = 0.35332273\n",
      "Iteration 221, loss = 0.35239978\n",
      "Iteration 222, loss = 0.35145849\n",
      "Iteration 223, loss = 0.35060467\n",
      "Iteration 224, loss = 0.34974376\n",
      "Iteration 225, loss = 0.34885167\n",
      "Iteration 226, loss = 0.34794051\n",
      "Iteration 227, loss = 0.34701298\n",
      "Iteration 228, loss = 0.34613088\n",
      "Iteration 229, loss = 0.34534958\n",
      "Iteration 230, loss = 0.34448380\n",
      "Iteration 231, loss = 0.34359929\n",
      "Iteration 232, loss = 0.34269869\n",
      "Iteration 233, loss = 0.34178436\n",
      "Iteration 234, loss = 0.34110791\n",
      "Iteration 235, loss = 0.34015141\n",
      "Iteration 236, loss = 0.33930397\n",
      "Iteration 237, loss = 0.33843958\n",
      "Iteration 238, loss = 0.33756066\n",
      "Iteration 239, loss = 0.33696781\n",
      "Iteration 240, loss = 0.33597372\n",
      "Iteration 241, loss = 0.33515009\n",
      "Iteration 242, loss = 0.33430993\n",
      "Iteration 243, loss = 0.33345561\n",
      "Iteration 244, loss = 0.33286676\n",
      "Iteration 245, loss = 0.33191654\n",
      "Iteration 246, loss = 0.33111734\n",
      "Iteration 247, loss = 0.33031156\n",
      "Iteration 248, loss = 0.32949208\n",
      "Iteration 249, loss = 0.32870295\n",
      "Iteration 250, loss = 0.32801650\n",
      "Iteration 251, loss = 0.32724813\n",
      "Iteration 252, loss = 0.32646294\n",
      "Iteration 253, loss = 0.32566326\n",
      "Iteration 254, loss = 0.32485120\n",
      "Iteration 255, loss = 0.32405815\n",
      "Iteration 256, loss = 0.32354664\n",
      "Iteration 257, loss = 0.32258446\n",
      "Iteration 258, loss = 0.32183069\n",
      "Iteration 259, loss = 0.32106233\n",
      "Iteration 260, loss = 0.32029221\n",
      "Iteration 261, loss = 0.31968660\n",
      "Iteration 262, loss = 0.31896892\n",
      "Iteration 263, loss = 0.31823480\n",
      "Iteration 264, loss = 0.31748648\n",
      "Iteration 265, loss = 0.31672599\n",
      "Iteration 266, loss = 0.31597972\n",
      "Iteration 267, loss = 0.31519401\n",
      "Iteration 268, loss = 0.31477449\n",
      "Iteration 269, loss = 0.31384078\n",
      "Iteration 270, loss = 0.31313852\n",
      "Iteration 271, loss = 0.31242220\n",
      "Iteration 272, loss = 0.31169378\n",
      "Iteration 273, loss = 0.31111982\n",
      "Iteration 274, loss = 0.31039860\n",
      "Iteration 275, loss = 0.30972428\n",
      "Iteration 276, loss = 0.30903522\n",
      "Iteration 277, loss = 0.30833345\n",
      "Iteration 278, loss = 0.30762081\n",
      "Iteration 279, loss = 0.30689893\n",
      "Iteration 280, loss = 0.30644622\n",
      "Iteration 281, loss = 0.30562115\n",
      "Iteration 282, loss = 0.30495890\n",
      "Iteration 283, loss = 0.30428384\n",
      "Iteration 284, loss = 0.30359776\n",
      "Iteration 285, loss = 0.30290232\n",
      "Iteration 286, loss = 0.30251847\n",
      "Iteration 287, loss = 0.30167434\n",
      "Iteration 288, loss = 0.30103697\n",
      "Iteration 289, loss = 0.30041425\n",
      "Iteration 290, loss = 0.29974238\n",
      "Iteration 291, loss = 0.29907876\n",
      "Iteration 292, loss = 0.29843694\n",
      "Iteration 293, loss = 0.29792579\n",
      "Iteration 294, loss = 0.29732368\n",
      "Iteration 295, loss = 0.29670738\n",
      "Iteration 296, loss = 0.29607878\n",
      "Iteration 297, loss = 0.29543961\n",
      "Iteration 298, loss = 0.29479140\n",
      "Iteration 299, loss = 0.29413553\n",
      "Iteration 300, loss = 0.29350925\n",
      "Iteration 301, loss = 0.29298467\n",
      "Iteration 302, loss = 0.29238812\n",
      "Iteration 303, loss = 0.29177998\n",
      "Iteration 304, loss = 0.29116184\n",
      "Iteration 305, loss = 0.29053518\n",
      "Iteration 306, loss = 0.28990130\n",
      "Iteration 307, loss = 0.28952715\n",
      "Iteration 308, loss = 0.28879256\n",
      "Iteration 309, loss = 0.28821772\n",
      "Iteration 310, loss = 0.28764964\n",
      "Iteration 311, loss = 0.28705157\n",
      "Iteration 312, loss = 0.28645410\n",
      "Iteration 313, loss = 0.28584887\n",
      "Iteration 314, loss = 0.28544861\n",
      "Iteration 315, loss = 0.28480845\n",
      "Iteration 316, loss = 0.28426611\n",
      "Iteration 317, loss = 0.28371150\n",
      "Iteration 318, loss = 0.28314626\n",
      "Iteration 319, loss = 0.28257187\n",
      "Iteration 320, loss = 0.28198965\n",
      "Iteration 321, loss = 0.28140080\n",
      "Iteration 322, loss = 0.28096730\n",
      "Iteration 323, loss = 0.28037699\n",
      "Iteration 324, loss = 0.27984563\n",
      "Iteration 325, loss = 0.27930354\n",
      "Iteration 326, loss = 0.27875217\n",
      "Iteration 327, loss = 0.27819285\n",
      "Iteration 328, loss = 0.27762675\n",
      "Iteration 329, loss = 0.27709604\n",
      "Iteration 330, loss = 0.27664498\n",
      "Iteration 331, loss = 0.27613467\n",
      "Iteration 332, loss = 0.27561360\n",
      "Iteration 333, loss = 0.27508323\n",
      "Iteration 334, loss = 0.27456285\n",
      "Iteration 335, loss = 0.27401412\n",
      "Iteration 336, loss = 0.27346922\n",
      "Iteration 337, loss = 0.27291881\n",
      "Iteration 338, loss = 0.27261881\n",
      "Iteration 339, loss = 0.27198238\n",
      "Iteration 340, loss = 0.27149525\n",
      "Iteration 341, loss = 0.27099762\n",
      "Iteration 342, loss = 0.27049090\n",
      "Iteration 343, loss = 0.26997633\n",
      "Iteration 344, loss = 0.26945505\n",
      "Iteration 345, loss = 0.26892807\n",
      "Iteration 346, loss = 0.26854569\n",
      "Iteration 347, loss = 0.26802029\n",
      "Iteration 348, loss = 0.26754858\n",
      "Iteration 349, loss = 0.26706701\n",
      "Iteration 350, loss = 0.26657686\n",
      "Iteration 351, loss = 0.26607934\n",
      "Iteration 352, loss = 0.26557550\n",
      "Iteration 353, loss = 0.26506630\n",
      "Iteration 354, loss = 0.26455848\n",
      "Iteration 355, loss = 0.26419160\n",
      "Iteration 356, loss = 0.26373703\n",
      "Iteration 357, loss = 0.26327288\n",
      "Iteration 358, loss = 0.26280044\n",
      "Iteration 359, loss = 0.26233405\n",
      "Iteration 360, loss = 0.26184842\n",
      "Iteration 361, loss = 0.26136307\n",
      "Iteration 362, loss = 0.26087273\n",
      "Iteration 363, loss = 0.26037904\n",
      "Iteration 364, loss = 0.26011655\n",
      "Iteration 365, loss = 0.25955065\n",
      "Iteration 366, loss = 0.25911680\n",
      "Iteration 367, loss = 0.25867372\n",
      "Iteration 368, loss = 0.25822260\n",
      "Iteration 369, loss = 0.25776455\n",
      "Iteration 370, loss = 0.25730054\n",
      "Iteration 371, loss = 0.25683145\n",
      "Iteration 372, loss = 0.25635808\n",
      "Iteration 373, loss = 0.25617120\n",
      "Iteration 374, loss = 0.25555052\n",
      "Iteration 375, loss = 0.25513116\n",
      "Iteration 376, loss = 0.25470314\n",
      "Iteration 377, loss = 0.25426759\n",
      "Iteration 378, loss = 0.25382555\n",
      "Iteration 379, loss = 0.25337791\n",
      "Iteration 380, loss = 0.25292552\n",
      "Iteration 381, loss = 0.25246911\n",
      "Iteration 382, loss = 0.25226575\n",
      "Iteration 383, loss = 0.25170506\n",
      "Iteration 384, loss = 0.25130702\n",
      "Iteration 385, loss = 0.25090004\n",
      "Iteration 386, loss = 0.25048526\n",
      "Iteration 387, loss = 0.25006371\n",
      "Iteration 388, loss = 0.24963632\n",
      "Iteration 389, loss = 0.24920392\n",
      "Iteration 390, loss = 0.24876728\n",
      "Iteration 391, loss = 0.24832705\n",
      "Iteration 392, loss = 0.24804272\n",
      "Iteration 393, loss = 0.24758042\n",
      "Iteration 394, loss = 0.24719278\n",
      "Iteration 395, loss = 0.24679535\n",
      "Iteration 396, loss = 0.24636219\n",
      "Iteration 397, loss = 0.24592394\n",
      "Iteration 398, loss = 0.24548832\n",
      "Iteration 399, loss = 0.24503729\n",
      "Iteration 400, loss = 0.24460578\n",
      "Iteration 401, loss = 0.24413545\n",
      "Iteration 402, loss = 0.24385125\n",
      "Iteration 403, loss = 0.24337844\n",
      "Iteration 404, loss = 0.24297873\n",
      "Iteration 405, loss = 0.24256719\n",
      "Iteration 406, loss = 0.24214532\n",
      "Iteration 407, loss = 0.24171449\n",
      "Iteration 408, loss = 0.24127594\n",
      "Iteration 409, loss = 0.24083077\n",
      "Iteration 410, loss = 0.24037999\n",
      "Iteration 411, loss = 0.23993368\n",
      "Iteration 412, loss = 0.23948725\n",
      "Iteration 413, loss = 0.23917749\n",
      "Iteration 414, loss = 0.23878119\n",
      "Iteration 415, loss = 0.23837534\n",
      "Iteration 416, loss = 0.23796346\n",
      "Iteration 417, loss = 0.23756064\n",
      "Iteration 418, loss = 0.23714193\n",
      "Iteration 419, loss = 0.23671706\n",
      "Iteration 420, loss = 0.23631500\n",
      "Iteration 421, loss = 0.23587281\n",
      "Iteration 422, loss = 0.23545087\n",
      "Iteration 423, loss = 0.23502981\n",
      "Iteration 424, loss = 0.23488870\n",
      "Iteration 425, loss = 0.23430944\n",
      "Iteration 426, loss = 0.23393344\n",
      "Iteration 427, loss = 0.23354867\n",
      "Iteration 428, loss = 0.23315627\n",
      "Iteration 429, loss = 0.23275728\n",
      "Iteration 430, loss = 0.23235265\n",
      "Iteration 431, loss = 0.23194514\n",
      "Iteration 432, loss = 0.23154922\n",
      "Iteration 433, loss = 0.23114056\n",
      "Iteration 434, loss = 0.23087968\n",
      "Iteration 435, loss = 0.23046775\n",
      "Iteration 436, loss = 0.23011511\n",
      "Iteration 437, loss = 0.22975341\n",
      "Iteration 438, loss = 0.22938381\n",
      "Iteration 439, loss = 0.22900734\n",
      "Iteration 440, loss = 0.22862496\n",
      "Iteration 441, loss = 0.22823751\n",
      "Iteration 442, loss = 0.22785106\n",
      "Iteration 443, loss = 0.22746903\n",
      "Iteration 444, loss = 0.22707848\n",
      "Iteration 445, loss = 0.22670807\n",
      "Iteration 446, loss = 0.22630643\n",
      "Iteration 447, loss = 0.22623090\n",
      "Iteration 448, loss = 0.22565200\n",
      "Iteration 449, loss = 0.22531105\n",
      "Iteration 450, loss = 0.22496246\n",
      "Iteration 451, loss = 0.22460721\n",
      "Iteration 452, loss = 0.22424621\n",
      "Iteration 453, loss = 0.22390315\n",
      "Iteration 454, loss = 0.22352797\n",
      "Iteration 455, loss = 0.22316506\n",
      "Iteration 456, loss = 0.22280945\n",
      "Iteration 457, loss = 0.22244366\n",
      "Iteration 458, loss = 0.22212529\n",
      "Iteration 459, loss = 0.22182822\n",
      "Iteration 460, loss = 0.22150670\n",
      "Iteration 461, loss = 0.22117747\n",
      "Iteration 462, loss = 0.22084149\n",
      "Iteration 463, loss = 0.22049966\n",
      "Iteration 464, loss = 0.22015277\n",
      "Iteration 465, loss = 0.21980884\n",
      "Iteration 466, loss = 0.21946372\n",
      "Iteration 467, loss = 0.21911286\n",
      "Iteration 468, loss = 0.21877485\n",
      "Iteration 469, loss = 0.21841843\n",
      "Iteration 470, loss = 0.21806609\n",
      "Iteration 471, loss = 0.21771105\n",
      "Iteration 472, loss = 0.21769659\n",
      "Iteration 473, loss = 0.21713248\n",
      "Iteration 474, loss = 0.21683000\n",
      "Iteration 475, loss = 0.21652013\n",
      "Iteration 476, loss = 0.21620380\n",
      "Iteration 477, loss = 0.21588183\n",
      "Iteration 478, loss = 0.21555497\n",
      "Iteration 479, loss = 0.21522391\n",
      "Iteration 480, loss = 0.21488924\n",
      "Iteration 481, loss = 0.21455153\n",
      "Iteration 482, loss = 0.21422164\n",
      "Iteration 483, loss = 0.21388493\n",
      "Iteration 484, loss = 0.21361434\n",
      "Iteration 485, loss = 0.21334089\n",
      "Iteration 486, loss = 0.21305567\n",
      "Iteration 487, loss = 0.21276301\n",
      "Iteration 488, loss = 0.21246385\n",
      "Iteration 489, loss = 0.21215898\n",
      "Iteration 490, loss = 0.21184915\n",
      "Iteration 491, loss = 0.21153504\n",
      "Iteration 492, loss = 0.21121723\n",
      "Iteration 493, loss = 0.21089627\n",
      "Iteration 494, loss = 0.21057264\n",
      "Iteration 495, loss = 0.21025123\n",
      "Iteration 496, loss = 0.20993446\n",
      "Iteration 497, loss = 0.20961174\n",
      "Iteration 498, loss = 0.20931179\n",
      "Iteration 499, loss = 0.20921024\n",
      "Iteration 500, loss = 0.20876729\n",
      "Iteration 501, loss = 0.20849194\n",
      "Iteration 502, loss = 0.20821020\n",
      "Iteration 503, loss = 0.20792284\n",
      "Iteration 504, loss = 0.20763058\n",
      "Iteration 505, loss = 0.20733407\n",
      "Iteration 506, loss = 0.20703388\n",
      "Iteration 507, loss = 0.20673054\n",
      "Iteration 508, loss = 0.20644915\n",
      "Iteration 509, loss = 0.20613091\n",
      "Iteration 510, loss = 0.20583667\n",
      "Iteration 511, loss = 0.20553530\n",
      "Iteration 512, loss = 0.20523338\n",
      "Iteration 513, loss = 0.20513720\n",
      "Iteration 514, loss = 0.20473109\n",
      "Iteration 515, loss = 0.20446911\n",
      "Iteration 516, loss = 0.20420105\n",
      "Iteration 517, loss = 0.20392764\n",
      "Iteration 518, loss = 0.20364956\n",
      "Iteration 519, loss = 0.20336742\n",
      "Iteration 520, loss = 0.20308175\n",
      "Iteration 521, loss = 0.20279305\n",
      "Iteration 522, loss = 0.20252834\n",
      "Iteration 523, loss = 0.20222232\n",
      "Iteration 524, loss = 0.20194541\n",
      "Iteration 525, loss = 0.20165530\n",
      "Iteration 526, loss = 0.20136781\n",
      "Iteration 527, loss = 0.20107820\n",
      "Iteration 528, loss = 0.20096201\n",
      "Iteration 529, loss = 0.20059735\n",
      "Iteration 530, loss = 0.20034693\n",
      "Iteration 531, loss = 0.20009088\n",
      "Iteration 532, loss = 0.19982989\n",
      "Iteration 533, loss = 0.19956458\n",
      "Iteration 534, loss = 0.19929551\n",
      "Iteration 535, loss = 0.19902317\n",
      "Iteration 536, loss = 0.19877611\n",
      "Iteration 537, loss = 0.19848388\n",
      "Iteration 538, loss = 0.19821779\n",
      "Iteration 539, loss = 0.19794700\n",
      "Iteration 540, loss = 0.19767440\n",
      "Iteration 541, loss = 0.19739954\n",
      "Iteration 542, loss = 0.19712277\n",
      "Iteration 543, loss = 0.19684442\n",
      "Iteration 544, loss = 0.19685291\n",
      "Iteration 545, loss = 0.19638312\n",
      "Iteration 546, loss = 0.19614338\n",
      "Iteration 547, loss = 0.19589852\n",
      "Iteration 548, loss = 0.19564916\n",
      "Iteration 549, loss = 0.19539586\n",
      "Iteration 550, loss = 0.19515962\n",
      "Iteration 551, loss = 0.19489227\n",
      "Iteration 552, loss = 0.19463543\n",
      "Iteration 553, loss = 0.19437622\n",
      "Iteration 554, loss = 0.19412651\n",
      "Iteration 555, loss = 0.19386784\n",
      "Iteration 556, loss = 0.19360686\n",
      "Iteration 557, loss = 0.19334394\n",
      "Iteration 558, loss = 0.19307936\n",
      "Iteration 559, loss = 0.19281341\n",
      "Iteration 560, loss = 0.19281748\n",
      "Iteration 561, loss = 0.19238583\n",
      "Iteration 562, loss = 0.19216252\n",
      "Iteration 563, loss = 0.19193383\n",
      "Iteration 564, loss = 0.19170039\n",
      "Iteration 565, loss = 0.19146278\n",
      "Iteration 566, loss = 0.19122151\n",
      "Iteration 567, loss = 0.19097706\n",
      "Iteration 568, loss = 0.19072984\n",
      "Iteration 569, loss = 0.19048024\n",
      "Iteration 570, loss = 0.19022860\n",
      "Iteration 571, loss = 0.18997521\n",
      "Iteration 572, loss = 0.18973621\n",
      "Iteration 573, loss = 0.18947629\n",
      "Iteration 574, loss = 0.18922593\n",
      "Iteration 575, loss = 0.18898287\n",
      "Iteration 576, loss = 0.18873345\n",
      "Iteration 577, loss = 0.18865954\n",
      "Iteration 578, loss = 0.18832152\n",
      "Iteration 579, loss = 0.18810693\n",
      "Iteration 580, loss = 0.18788745\n",
      "Iteration 581, loss = 0.18766368\n",
      "Iteration 582, loss = 0.18743612\n",
      "Iteration 583, loss = 0.18720525\n",
      "Iteration 584, loss = 0.18697149\n",
      "Iteration 585, loss = 0.18673523\n",
      "Iteration 586, loss = 0.18649680\n",
      "Iteration 587, loss = 0.18625652\n",
      "Iteration 588, loss = 0.18602785\n",
      "Iteration 589, loss = 0.18578292\n",
      "Iteration 590, loss = 0.18554346\n",
      "Iteration 591, loss = 0.18531457\n",
      "Iteration 592, loss = 0.18507186\n",
      "Iteration 593, loss = 0.18483360\n",
      "Iteration 594, loss = 0.18462005\n",
      "Iteration 595, loss = 0.18444052\n",
      "Iteration 596, loss = 0.18423581\n",
      "Iteration 597, loss = 0.18402647\n",
      "Iteration 598, loss = 0.18381305\n",
      "Iteration 599, loss = 0.18359604\n",
      "Iteration 600, loss = 0.18337588\n",
      "Iteration 601, loss = 0.18315297\n",
      "Iteration 602, loss = 0.18292767\n",
      "Iteration 603, loss = 0.18270030\n",
      "Iteration 604, loss = 0.18247237\n",
      "Iteration 605, loss = 0.18225142\n",
      "Iteration 606, loss = 0.18202410\n",
      "Iteration 607, loss = 0.18179521\n",
      "Iteration 608, loss = 0.18157127\n",
      "Iteration 609, loss = 0.18134448\n",
      "Iteration 610, loss = 0.18111681\n",
      "Iteration 611, loss = 0.18088791\n",
      "Iteration 612, loss = 0.18065799\n",
      "Iteration 613, loss = 0.18060443\n",
      "Iteration 614, loss = 0.18027898\n",
      "Iteration 615, loss = 0.18008217\n",
      "Iteration 616, loss = 0.17988124\n",
      "Iteration 617, loss = 0.17967665\n",
      "Iteration 618, loss = 0.17946887\n",
      "Iteration 619, loss = 0.17925827\n",
      "Iteration 620, loss = 0.17904522\n",
      "Iteration 621, loss = 0.17884520\n",
      "Iteration 622, loss = 0.17862343\n",
      "Iteration 623, loss = 0.17840923\n",
      "Iteration 624, loss = 0.17819320\n",
      "Iteration 625, loss = 0.17798201\n",
      "Iteration 626, loss = 0.17776701\n",
      "Iteration 627, loss = 0.17755133\n",
      "Iteration 628, loss = 0.17733425\n",
      "Iteration 629, loss = 0.17711600\n",
      "Iteration 630, loss = 0.17689675\n",
      "Iteration 631, loss = 0.17667670\n",
      "Iteration 632, loss = 0.17671168\n",
      "Iteration 633, loss = 0.17632434\n",
      "Iteration 634, loss = 0.17614058\n",
      "Iteration 635, loss = 0.17595253\n",
      "Iteration 636, loss = 0.17576067\n",
      "Iteration 637, loss = 0.17556545\n",
      "Iteration 638, loss = 0.17536728\n",
      "Iteration 639, loss = 0.17516651\n",
      "Iteration 640, loss = 0.17496349\n",
      "Iteration 641, loss = 0.17475849\n",
      "Iteration 642, loss = 0.17455179\n",
      "Iteration 643, loss = 0.17434363\n",
      "Iteration 644, loss = 0.17413423\n",
      "Iteration 645, loss = 0.17392377\n",
      "Iteration 646, loss = 0.17371429\n",
      "Iteration 647, loss = 0.17351008\n",
      "Iteration 648, loss = 0.17330154\n",
      "Iteration 649, loss = 0.17309211\n",
      "Iteration 650, loss = 0.17289564\n",
      "Iteration 651, loss = 0.17268476\n",
      "Iteration 652, loss = 0.17254956\n",
      "Iteration 653, loss = 0.17237372\n",
      "Iteration 654, loss = 0.17219385\n",
      "Iteration 655, loss = 0.17201040\n",
      "Iteration 656, loss = 0.17182379\n",
      "Iteration 657, loss = 0.17163440\n",
      "Iteration 658, loss = 0.17144256\n",
      "Iteration 659, loss = 0.17124860\n",
      "Iteration 660, loss = 0.17105277\n",
      "Iteration 661, loss = 0.17085534\n",
      "Iteration 662, loss = 0.17065652\n",
      "Iteration 663, loss = 0.17045652\n",
      "Iteration 664, loss = 0.17025552\n",
      "Iteration 665, loss = 0.17006573\n",
      "Iteration 666, loss = 0.16986038\n",
      "Iteration 667, loss = 0.16966119\n",
      "Iteration 668, loss = 0.16946685\n",
      "Iteration 669, loss = 0.16926954\n",
      "Iteration 670, loss = 0.16907208\n",
      "Iteration 671, loss = 0.16887374\n",
      "Iteration 672, loss = 0.16873031\n",
      "Iteration 673, loss = 0.16854745\n",
      "Iteration 674, loss = 0.16837797\n",
      "Iteration 675, loss = 0.16820489\n",
      "Iteration 676, loss = 0.16802863\n",
      "Iteration 677, loss = 0.16784956\n",
      "Iteration 678, loss = 0.16766801\n",
      "Iteration 679, loss = 0.16748429\n",
      "Iteration 680, loss = 0.16729868\n",
      "Iteration 681, loss = 0.16711142\n",
      "Iteration 682, loss = 0.16692274\n",
      "Iteration 683, loss = 0.16673282\n",
      "Iteration 684, loss = 0.16655288\n",
      "Iteration 685, loss = 0.16635882\n",
      "Iteration 686, loss = 0.16616996\n",
      "Iteration 687, loss = 0.16598010\n",
      "Iteration 688, loss = 0.16580059\n",
      "Iteration 689, loss = 0.16560668\n",
      "Iteration 690, loss = 0.16541837\n",
      "Iteration 691, loss = 0.16522920\n",
      "Iteration 692, loss = 0.16503932\n",
      "Iteration 693, loss = 0.16488629\n",
      "Iteration 694, loss = 0.16472699\n",
      "Iteration 695, loss = 0.16456492\n",
      "Iteration 696, loss = 0.16439948\n",
      "Iteration 697, loss = 0.16423107\n",
      "Iteration 698, loss = 0.16406003\n",
      "Iteration 699, loss = 0.16388668\n",
      "Iteration 700, loss = 0.16371130\n",
      "Iteration 701, loss = 0.16353414\n",
      "Iteration 702, loss = 0.16335544\n",
      "Iteration 703, loss = 0.16317540\n",
      "Iteration 704, loss = 0.16301599\n",
      "Iteration 705, loss = 0.16282040\n",
      "Iteration 706, loss = 0.16264093\n",
      "Iteration 707, loss = 0.16246380\n",
      "Iteration 708, loss = 0.16228723\n",
      "Iteration 709, loss = 0.16210852\n",
      "Iteration 710, loss = 0.16192879\n",
      "Iteration 711, loss = 0.16174820\n",
      "Iteration 712, loss = 0.16156688\n",
      "Iteration 713, loss = 0.16138497\n",
      "Iteration 714, loss = 0.16120256\n",
      "Iteration 715, loss = 0.16103734\n",
      "Iteration 716, loss = 0.16090250\n",
      "Iteration 717, loss = 0.16074705\n",
      "Iteration 718, loss = 0.16058852\n",
      "Iteration 719, loss = 0.16042725\n",
      "Iteration 720, loss = 0.16026358\n",
      "Iteration 721, loss = 0.16009779\n",
      "Iteration 722, loss = 0.15993014\n",
      "Iteration 723, loss = 0.15976085\n",
      "Iteration 724, loss = 0.15960618\n",
      "Iteration 725, loss = 0.15942620\n",
      "Iteration 726, loss = 0.15925659\n",
      "Iteration 727, loss = 0.15908571\n",
      "Iteration 728, loss = 0.15891375\n",
      "Iteration 729, loss = 0.15875073\n",
      "Iteration 730, loss = 0.15857506\n",
      "Iteration 731, loss = 0.15840404\n",
      "Iteration 732, loss = 0.15823213\n",
      "Iteration 733, loss = 0.15805945\n",
      "Iteration 734, loss = 0.15788613\n",
      "Iteration 735, loss = 0.15771228\n",
      "Iteration 736, loss = 0.15753799\n",
      "Iteration 737, loss = 0.15737926\n",
      "Iteration 738, loss = 0.15723079\n",
      "Iteration 739, loss = 0.15708705\n",
      "Iteration 740, loss = 0.15694131\n",
      "Iteration 741, loss = 0.15679241\n",
      "Iteration 742, loss = 0.15664070\n",
      "Iteration 743, loss = 0.15648651\n",
      "Iteration 744, loss = 0.15633013\n",
      "Iteration 745, loss = 0.15617181\n",
      "Iteration 746, loss = 0.15601180\n",
      "Iteration 747, loss = 0.15585031\n",
      "Iteration 748, loss = 0.15568752\n",
      "Iteration 749, loss = 0.15552362\n",
      "Iteration 750, loss = 0.15535875\n",
      "Iteration 751, loss = 0.15519306\n",
      "Iteration 752, loss = 0.15502668\n",
      "Iteration 753, loss = 0.15485971\n",
      "Iteration 754, loss = 0.15471055\n",
      "Iteration 755, loss = 0.15453183\n",
      "Iteration 756, loss = 0.15436682\n",
      "Iteration 757, loss = 0.15420819\n",
      "Iteration 758, loss = 0.15404256\n",
      "Iteration 759, loss = 0.15387925\n",
      "Iteration 760, loss = 0.15371532\n",
      "Iteration 761, loss = 0.15355086\n",
      "Iteration 762, loss = 0.15347682\n",
      "Iteration 763, loss = 0.15328039\n",
      "Iteration 764, loss = 0.15314016\n",
      "Iteration 765, loss = 0.15299709\n",
      "Iteration 766, loss = 0.15285149\n",
      "Iteration 767, loss = 0.15270365\n",
      "Iteration 768, loss = 0.15255385\n",
      "Iteration 769, loss = 0.15240231\n",
      "Iteration 770, loss = 0.15224924\n",
      "Iteration 771, loss = 0.15209485\n",
      "Iteration 772, loss = 0.15193930\n",
      "Iteration 773, loss = 0.15178274\n",
      "Iteration 774, loss = 0.15162532\n",
      "Iteration 775, loss = 0.15146717\n",
      "Iteration 776, loss = 0.15132493\n",
      "Iteration 777, loss = 0.15115615\n",
      "Iteration 778, loss = 0.15099942\n",
      "Iteration 779, loss = 0.15084281\n",
      "Iteration 780, loss = 0.15069107\n",
      "Iteration 781, loss = 0.15053561\n",
      "Iteration 782, loss = 0.15037947\n",
      "Iteration 783, loss = 0.15022275\n",
      "Iteration 784, loss = 0.15006555\n",
      "Iteration 785, loss = 0.14990795\n",
      "Iteration 786, loss = 0.14975040\n",
      "Iteration 787, loss = 0.14964864\n",
      "Iteration 788, loss = 0.14951434\n",
      "Iteration 789, loss = 0.14937739\n",
      "Iteration 790, loss = 0.14923810\n",
      "Iteration 791, loss = 0.14909673\n",
      "Iteration 792, loss = 0.14895353\n",
      "Iteration 793, loss = 0.14880872\n",
      "Iteration 794, loss = 0.14866248\n",
      "Iteration 795, loss = 0.14851501\n",
      "Iteration 796, loss = 0.14836645\n",
      "Iteration 797, loss = 0.14821697\n",
      "Iteration 798, loss = 0.14806731\n",
      "Iteration 799, loss = 0.14792242\n",
      "Iteration 800, loss = 0.14777370\n",
      "Iteration 801, loss = 0.14762420\n",
      "Iteration 802, loss = 0.14747403\n",
      "Iteration 803, loss = 0.14732329\n",
      "Iteration 804, loss = 0.14718150\n",
      "Iteration 805, loss = 0.14702709\n",
      "Iteration 806, loss = 0.14687797\n",
      "Iteration 807, loss = 0.14672831\n",
      "Iteration 808, loss = 0.14657819\n",
      "Iteration 809, loss = 0.14642770\n",
      "Iteration 810, loss = 0.14627691\n",
      "Iteration 811, loss = 0.14612722\n",
      "Iteration 812, loss = 0.14609431\n",
      "Iteration 813, loss = 0.14588657\n",
      "Iteration 814, loss = 0.14576050\n",
      "Iteration 815, loss = 0.14563175\n",
      "Iteration 816, loss = 0.14550061\n",
      "Iteration 817, loss = 0.14536737\n",
      "Iteration 818, loss = 0.14523226\n",
      "Iteration 819, loss = 0.14509549\n",
      "Iteration 820, loss = 0.14495727\n",
      "Iteration 821, loss = 0.14481778\n",
      "Iteration 822, loss = 0.14467718\n",
      "Iteration 823, loss = 0.14453560\n",
      "Iteration 824, loss = 0.14439319\n",
      "Iteration 825, loss = 0.14425006\n",
      "Iteration 826, loss = 0.14410631\n",
      "Iteration 827, loss = 0.14396204\n",
      "Iteration 828, loss = 0.14381734\n",
      "Iteration 829, loss = 0.14367227\n",
      "Iteration 830, loss = 0.14352692\n",
      "Iteration 831, loss = 0.14339084\n",
      "Iteration 832, loss = 0.14324184\n",
      "Iteration 833, loss = 0.14309861\n",
      "Iteration 834, loss = 0.14295503\n",
      "Iteration 835, loss = 0.14281901\n",
      "Iteration 836, loss = 0.14267326\n",
      "Iteration 837, loss = 0.14253161\n",
      "Iteration 838, loss = 0.14247812\n",
      "Iteration 839, loss = 0.14229848\n",
      "Iteration 840, loss = 0.14217760\n",
      "Iteration 841, loss = 0.14205425\n",
      "Iteration 842, loss = 0.14192870\n",
      "Iteration 843, loss = 0.14180121\n",
      "Iteration 844, loss = 0.14167200\n",
      "Iteration 845, loss = 0.14154127\n",
      "Iteration 846, loss = 0.14140921\n",
      "Iteration 847, loss = 0.14127597\n",
      "Iteration 848, loss = 0.14114170\n",
      "Iteration 849, loss = 0.14100654\n",
      "Iteration 850, loss = 0.14087061\n",
      "Iteration 851, loss = 0.14073402\n",
      "Iteration 852, loss = 0.14059686\n",
      "Iteration 853, loss = 0.14045922\n",
      "Iteration 854, loss = 0.14032118\n",
      "Iteration 855, loss = 0.14018348\n",
      "Iteration 856, loss = 0.14005011\n",
      "Iteration 857, loss = 0.13991378\n",
      "Iteration 858, loss = 0.13977703\n",
      "Iteration 859, loss = 0.13963992\n",
      "Iteration 860, loss = 0.13950570\n",
      "Iteration 861, loss = 0.13937080\n",
      "Iteration 862, loss = 0.13923550\n",
      "Iteration 863, loss = 0.13909981\n",
      "Iteration 864, loss = 0.13896381\n",
      "Iteration 865, loss = 0.13889778\n",
      "Iteration 866, loss = 0.13873979\n",
      "Iteration 867, loss = 0.13862383\n",
      "Iteration 868, loss = 0.13850561\n",
      "Iteration 869, loss = 0.13838539\n",
      "Iteration 870, loss = 0.13826338\n",
      "Iteration 871, loss = 0.13813981\n",
      "Iteration 872, loss = 0.13801484\n",
      "Iteration 873, loss = 0.13788865\n",
      "Iteration 874, loss = 0.13776138\n",
      "Iteration 875, loss = 0.13763318\n",
      "Iteration 876, loss = 0.13750416\n",
      "Iteration 877, loss = 0.13737444\n",
      "Iteration 878, loss = 0.13724411\n",
      "Iteration 879, loss = 0.13711326\n",
      "Iteration 880, loss = 0.13698299\n",
      "Iteration 881, loss = 0.13685597\n",
      "Iteration 882, loss = 0.13672641\n",
      "Iteration 883, loss = 0.13659636\n",
      "Iteration 884, loss = 0.13646589\n",
      "Iteration 885, loss = 0.13633507\n",
      "Iteration 886, loss = 0.13621094\n",
      "Iteration 887, loss = 0.13607822\n",
      "Iteration 888, loss = 0.13594908\n",
      "Iteration 889, loss = 0.13581955\n",
      "Iteration 890, loss = 0.13568972\n",
      "Iteration 891, loss = 0.13555962\n",
      "Iteration 892, loss = 0.13542933\n",
      "Iteration 893, loss = 0.13540164\n",
      "Iteration 894, loss = 0.13521458\n",
      "Iteration 895, loss = 0.13510356\n",
      "Iteration 896, loss = 0.13499046\n",
      "Iteration 897, loss = 0.13487551\n",
      "Iteration 898, loss = 0.13475893\n",
      "Iteration 899, loss = 0.13464090\n",
      "Iteration 900, loss = 0.13452158\n",
      "Iteration 901, loss = 0.13440114\n",
      "Iteration 902, loss = 0.13427972\n",
      "Iteration 903, loss = 0.13415743\n",
      "Iteration 904, loss = 0.13403438\n",
      "Iteration 905, loss = 0.13391069\n",
      "Iteration 906, loss = 0.13379822\n",
      "Iteration 907, loss = 0.13366708\n",
      "Iteration 908, loss = 0.13354422\n",
      "Iteration 909, loss = 0.13342079\n",
      "Iteration 910, loss = 0.13329686\n",
      "Iteration 911, loss = 0.13317252\n",
      "Iteration 912, loss = 0.13305581\n",
      "Iteration 913, loss = 0.13292815\n",
      "Iteration 914, loss = 0.13280518\n",
      "Iteration 915, loss = 0.13268179\n",
      "Iteration 916, loss = 0.13255805\n",
      "Iteration 917, loss = 0.13243402\n",
      "Iteration 918, loss = 0.13230976\n",
      "Iteration 919, loss = 0.13218530\n",
      "Iteration 920, loss = 0.13206070\n",
      "Iteration 921, loss = 0.13193954\n",
      "Iteration 922, loss = 0.13192906\n",
      "Iteration 923, loss = 0.13173771\n",
      "Iteration 924, loss = 0.13163349\n",
      "Iteration 925, loss = 0.13152714\n",
      "Iteration 926, loss = 0.13141891\n",
      "Iteration 927, loss = 0.13130901\n",
      "Iteration 928, loss = 0.13119761\n",
      "Iteration 929, loss = 0.13108490\n",
      "Iteration 930, loss = 0.13097103\n",
      "Iteration 931, loss = 0.13085614\n",
      "Iteration 932, loss = 0.13074035\n",
      "Iteration 933, loss = 0.13062378\n",
      "Iteration 934, loss = 0.13050653\n",
      "Iteration 935, loss = 0.13038869\n",
      "Iteration 936, loss = 0.13027034\n",
      "Iteration 937, loss = 0.13015156\n",
      "Iteration 938, loss = 0.13003242\n",
      "Iteration 939, loss = 0.12991298\n",
      "Iteration 940, loss = 0.12979328\n",
      "Iteration 941, loss = 0.12967339\n",
      "Iteration 942, loss = 0.12955334\n",
      "Iteration 943, loss = 0.12944489\n",
      "Iteration 944, loss = 0.12931790\n",
      "Iteration 945, loss = 0.12919971\n",
      "Iteration 946, loss = 0.12908128\n",
      "Iteration 947, loss = 0.12896649\n",
      "Iteration 948, loss = 0.12884881\n",
      "Iteration 949, loss = 0.12873201\n",
      "Iteration 950, loss = 0.12861491\n",
      "Iteration 951, loss = 0.12849757\n",
      "Iteration 952, loss = 0.12852617\n",
      "Iteration 953, loss = 0.12830399\n",
      "Iteration 954, loss = 0.12820385\n",
      "Iteration 955, loss = 0.12810180\n",
      "Iteration 956, loss = 0.12799805\n",
      "Iteration 957, loss = 0.12789279\n",
      "Iteration 958, loss = 0.12778618\n",
      "Iteration 959, loss = 0.12767839\n",
      "Iteration 960, loss = 0.12756956\n",
      "Iteration 961, loss = 0.12745981\n",
      "Iteration 962, loss = 0.12734925\n",
      "Iteration 963, loss = 0.12723799\n",
      "Iteration 964, loss = 0.12712612\n",
      "Iteration 965, loss = 0.12701372\n",
      "Iteration 966, loss = 0.12690087\n",
      "Iteration 967, loss = 0.12678764\n",
      "Iteration 968, loss = 0.12667408\n",
      "Iteration 969, loss = 0.12656025\n",
      "Iteration 970, loss = 0.12644720\n",
      "Iteration 971, loss = 0.12633671\n",
      "Iteration 972, loss = 0.12622435\n",
      "Iteration 973, loss = 0.12611168\n",
      "Iteration 974, loss = 0.12599875\n",
      "Iteration 975, loss = 0.12588562\n",
      "Iteration 976, loss = 0.12577689\n",
      "Iteration 977, loss = 0.12566355\n",
      "Iteration 978, loss = 0.12555201\n",
      "Iteration 979, loss = 0.12544019\n",
      "Iteration 980, loss = 0.12532814\n",
      "Iteration 981, loss = 0.12521592\n",
      "Iteration 982, loss = 0.12517326\n",
      "Iteration 983, loss = 0.12503064\n",
      "Iteration 984, loss = 0.12493488\n",
      "Iteration 985, loss = 0.12483733\n",
      "Iteration 986, loss = 0.12473821\n",
      "Iteration 987, loss = 0.12463767\n",
      "Iteration 988, loss = 0.12453589\n",
      "Iteration 989, loss = 0.12443301\n",
      "Iteration 990, loss = 0.12432915\n",
      "Iteration 991, loss = 0.12422443\n",
      "Iteration 992, loss = 0.12411896\n",
      "Iteration 993, loss = 0.12401284\n",
      "Iteration 994, loss = 0.12390615\n",
      "Iteration 995, loss = 0.12379896\n",
      "Iteration 996, loss = 0.12369136\n",
      "Iteration 997, loss = 0.12358339\n",
      "Iteration 998, loss = 0.12347512\n",
      "Iteration 999, loss = 0.12338129\n",
      "Iteration 1000, loss = 0.12326236\n",
      "Iteration 1001, loss = 0.12315537\n",
      "Iteration 1002, loss = 0.12304805\n",
      "Iteration 1003, loss = 0.12294046\n",
      "Iteration 1004, loss = 0.12283813\n",
      "Iteration 1005, loss = 0.12272909\n",
      "Iteration 1006, loss = 0.12262282\n",
      "Iteration 1007, loss = 0.12251625\n",
      "Iteration 1008, loss = 0.12240942\n",
      "Iteration 1009, loss = 0.12230238\n",
      "Iteration 1010, loss = 0.12219516\n",
      "Iteration 1011, loss = 0.12208781\n",
      "Iteration 1012, loss = 0.12198037\n",
      "Iteration 1013, loss = 0.12187285\n",
      "Iteration 1014, loss = 0.12192771\n",
      "Iteration 1015, loss = 0.12169519\n",
      "Iteration 1016, loss = 0.12160351\n",
      "Iteration 1017, loss = 0.12151021\n",
      "Iteration 1018, loss = 0.12141547\n",
      "Iteration 1019, loss = 0.12131944\n",
      "Iteration 1020, loss = 0.12122227\n",
      "Iteration 1021, loss = 0.12112410\n",
      "Iteration 1022, loss = 0.12102504\n",
      "Iteration 1023, loss = 0.12092520\n",
      "Iteration 1024, loss = 0.12082468\n",
      "Iteration 1025, loss = 0.12072356\n",
      "Iteration 1026, loss = 0.12062192\n",
      "Iteration 1027, loss = 0.12052691\n",
      "Iteration 1028, loss = 0.12042165\n",
      "Iteration 1029, loss = 0.12032067\n",
      "Iteration 1030, loss = 0.12021923\n",
      "Iteration 1031, loss = 0.12011738\n",
      "Iteration 1032, loss = 0.12001520\n",
      "Iteration 1033, loss = 0.11991273\n",
      "Iteration 1034, loss = 0.11981088\n",
      "Iteration 1035, loss = 0.11971133\n",
      "Iteration 1036, loss = 0.11961005\n",
      "Iteration 1037, loss = 0.11950846\n",
      "Iteration 1038, loss = 0.11940661\n",
      "Iteration 1039, loss = 0.11930454\n",
      "Iteration 1040, loss = 0.11920230\n",
      "Iteration 1041, loss = 0.11909991\n",
      "Iteration 1042, loss = 0.11899742\n",
      "Iteration 1043, loss = 0.11889485\n",
      "Iteration 1044, loss = 0.11879222\n",
      "Iteration 1045, loss = 0.11869730\n",
      "Iteration 1046, loss = 0.11868363\n",
      "Iteration 1047, loss = 0.11852569\n",
      "Iteration 1048, loss = 0.11843973\n",
      "Iteration 1049, loss = 0.11835209\n",
      "Iteration 1050, loss = 0.11826296\n",
      "Iteration 1051, loss = 0.11817249\n",
      "Iteration 1052, loss = 0.11808085\n",
      "Iteration 1053, loss = 0.11798816\n",
      "Iteration 1054, loss = 0.11789454\n",
      "Iteration 1055, loss = 0.11780010\n",
      "Iteration 1056, loss = 0.11770495\n",
      "Iteration 1057, loss = 0.11760917\n",
      "Iteration 1058, loss = 0.11751284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[1 0]\n",
      "[(2, 5), (5, 2), (2, 1)]\n",
      "[array([[ 0.19688505, -0.14291777,  0.45745354,  0.89360043,  0.56041658],\n",
      "       [ 0.58732717,  0.4541344 ,  0.91978911, -0.61576792,  0.46402032]]), array([[-0.52583864, -0.58516125],\n",
      "       [ 0.93940502,  0.72530124],\n",
      "       [-1.01709281, -0.46140573],\n",
      "       [ 0.79231453,  0.66981059],\n",
      "       [-1.01287091,  0.48606521]]), array([[-2.01803825],\n",
      "       [-0.4842527 ]])]\n"
     ]
    }
   ],
   "source": [
    "# 分类器\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# 回归\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[0.,0.],\n",
    "             [1.,1.]])\n",
    "\n",
    "Y = np.array([0,1])\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "'''\n",
    "solver选择优化算法 ,sgd 随机梯度下降算法\n",
    "alpha 正则项系数\n",
    "activation 激活函数，隐藏层（hidden layer）中使用\n",
    "hidden_layer_sizes = (5, 2),第一个隐藏层有5个节点，第二个隐藏层有2个节点\n",
    "max_iter 最大迭代次数\n",
    "tol->tolerance 对于连续10次loss差值\n",
    "verbose是否输出中途信息\n",
    "'''\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, activation='relu',\n",
    "                    hidden_layer_sizes=(5, 2), max_iter=2000, tol=1e-4, verbose=True)\n",
    "clf.fit(X,Y)\n",
    "\n",
    "preficted_value = clf.predict([[2,2],\n",
    "                               [-1,-2]])\n",
    "\n",
    "print(preficted_value)\n",
    "print([coef.shape for coef in clf.coefs_])              \n",
    "print([coef for coef in clf.coefs_])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
